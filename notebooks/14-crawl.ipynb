{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web scraping and crawling\n",
    "Uses the *BeautifulSoup* external package.\n",
    "[BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup / Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "from util import utility\n",
    "from settings import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting started"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Website to work with, i.e. to scrape info from and crawl over it - Ultimate Classic Rock.\n",
    "The starting URL refers to articles about Paul McCartney."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_url = 'https://ultimateclassicrock.com/search/?s=paul%20mccartney'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create `Response` object from GET request, using `requests.get(<url>, allow_redirects=False)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = requests.get(start_url, allow_redirects=False)\n",
    "print(type(response))\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get response text from `Response` object, using `<response>.text`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response_text = response.text\n",
    "print(response_text[:4000])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get BeautifulSoup object from response text, using `BeautifulSoup(<response text>, 'html.parser')`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response_text, 'html.parser')\n",
    "print(type(soup))\n",
    "print(soup)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `get_soup(url)` function for getting a BeautifulSoup object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Returns BeautifulSoup object from the corresponding URL, passed as a string.\n",
    "    Creates Response object from HTTP GET request, using requests.get(<url string>, allow_redirects=False),\n",
    "    and then uses the text field of the Response object and the 'html.parser' to create the BeautifulSoup object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Response object from HTTP GET request; assume that no redirection is allowed (allow_redirects=False)\n",
    "    response = requests.get(url, allow_redirects=False)\n",
    "\n",
    "    # Get text from the Response object, using <response>.text\n",
    "    response_text = response.text\n",
    "\n",
    "    # Create and return the corresponding BeautifulSoup object from the response text; use features='html.parser'\n",
    "    return BeautifulSoup(response_text, features='html.parser')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_soup(url)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = get_soup(start_url)\n",
    "# print(type(soup))\n",
    "# print(str(soup))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save BeautifulSoup object to an HTML file, using `<Path-file-object>.write_text(str(<BeautifulSoup object>), encoding='utf-8', errors='replace')`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup_file = DATA_DIR / 'soup.html'\n",
    "soup_file.write_text(str(soup), encoding='utf-8', errors='replace')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<BeautifulSoup object>.find('<tag>')`; e.g., find the first `<span>` tag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "span_first = soup.find('span')\n",
    "print(span_first)\n",
    "print()\n",
    "print(type(span_first))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<BeautifulSoup object>.find('<tag>').find('<nested tag>')`; e.g., find the `<a>` tag in an `<article>` tag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_first = soup.find('article').find('a')\n",
    "print(a_first)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate getting a tag with specific attributes using `<BeautifulSoup object>.find('<tag>', {'<attribute>': '<value>'})`; e.g., find the `<visually-hidden>` tag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visually_hidden = soup.find('span', {'class': 'visually-hidden'})\n",
    "print(visually_hidden)\n",
    "# article_date = div_content.findNext('div', {'class': 'auth-date'}).findNext('time')\n",
    "# print(article_date)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate getting values of tag attributes, e.g. `<BeautifulSoup object>.find('<tag>').text` for an `<a>` tag and for a `<visually-hidden>` tag.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_first = soup.find('article').find('a').text\n",
    "print(a_first)\n",
    "visually_hidden = soup.find('span', {'class': 'visually-hidden'}).text\n",
    "print(visually_hidden)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<BeautifulSoup object>.find_all(<tag>)`, e.g. for the `<article>` tag; returns a `ResultSet` object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = soup.findAll('article')\n",
    "print(type(articles))\n",
    "print(articles)\n",
    "print()\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following lines demonstrate that getting the soup with `requests.get()` does not capture all tags (those filled with JavaScript, e.g. `<time>`). That's when using `selenium.webdriver` is better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article_date = soup.find('div', {'class': 'auth-date'})\n",
    "print(article_date)\n",
    "article_date = soup.find('div', {'class': 'auth-date'}).findNext('time').text\n",
    "print(article_date)\n",
    "print(len(article_date))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Selenium version, needed for extracting the `<time>` tag info."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Before running the following line, make sure to download and unzip chromedriver and put chromedriver.exe\n",
    "# in the Scripts subfolder of your Python installation folder,\n",
    "# e.g. C:\\Users\\Vladan\\AppData\\Local\\Programs\\Python\\Python310\\Scripts.\n",
    "# To find the Python installation folder from a Python script or notebook, just run:\n",
    "#   import sys\n",
    "#   sys.executable\n",
    "# The driver should be downloaded from https://chromedriver.chromium.org/downloads.\n",
    "# Then you need not provide the path of the driver, just run: driver = webdriver.Chrome().\n",
    "# (Adapted from https://stackoverflow.com/a/60062969/1899061.)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(start_url)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the BeautifulSoup object to an HTML file, using `<Path-file-object>.write_text(str(<BeautifulSoup object>), encoding='utf-8', errors='replace')`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup_file = DATA_DIR / 'soup.html'\n",
    "soup_file.write_text(str(soup), encoding='utf-8', errors='replace')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `get_soup_selenium(url)` function for getting a BeautifulSoup object using the `selenium` package instead of `requests`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_soup_selenium(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Returns BeautifulSoup object from the corresponding URL, passed as a string.\n",
    "    Makes an HTTP GET request, using driver = webdriver.Chrome() from the selenium package and its driver.get(url).\n",
    "    Then uses the page_source field of the driver object and the 'html.parser' to create and return the BeautifulSoup o.\n",
    "    \"\"\"\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_soup_selenium(url)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = get_soup_selenium(start_url)\n",
    "print(soup.find('article').find('div', {'class': 'content'}).find('a').text)\n",
    "# a_first = soup.find('article').find('a').text\n",
    "# print(a_first)\n",
    "# print(type(soup))\n",
    "# print(str(soup))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate occasional anomalies in the ResultSet returned by `<BeautifulSoup object>.find_all(<tag>)`; note that they may be appearing only in the `selenium` version, not in the `requests` version."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The following lines show that there are 11 articles on the page, not 10.\n",
    "# The 11th one is something else, not visible on the page at the first glance and should be eliminated from\n",
    "# further processing.\n",
    "articles = soup.findAll('article')\n",
    "print(type(articles))\n",
    "print(articles)\n",
    "print()\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    print()\n",
    "\n",
    "print(len(articles))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line shows an anomaly in the articles `ResultSet`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(articles[len(articles) - 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare it to any of the other results from the `ResultSet` returned by `<BeautifulSoup object>.find_all(<tag>)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(articles[0])\n",
    "print()\n",
    "print(articles[len(articles) - 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate different ways of getting an attribute value for a tag (a `bs4.element.Tag` object):\n",
    "- `<tag>.find('<subtag>')`, filtered with `<{'class': \"<class name>\"}>\n",
    "- `<tag>.find('<tag>')['<attr>']`\n",
    "- `<tag>.find('<subtag>').get('<attribute>')`\n",
    "- `<tag>.find('<subtag>').<attribute>,... (<attribute>: e.g. text)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article = articles[0]\n",
    "print(type(article))  # bs4.element.Tag\n",
    "print()\n",
    "\n",
    "print(article.find('a', {'class': 'theframe'}))\n",
    "print(article.find('a')['class'])\n",
    "print(article.find('a').get('class'))\n",
    "print(article.find('a').attrs['class'])\n",
    "print(article.find('a', {'class': 'theframe'}).text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<tag>.find_next_siblings()` (returns all `<tag>`'s siblings) and `<tag>.find_next_sibling()` (returns just the first one)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_articles_div = soup.find('div', {'class': 'rowline clearfix'})\n",
    "# print(str(all_articles_div)[0:2000])\n",
    "# print(all_articles_div.findAll)\n",
    "# print(all_articles_div.find('span').findNext)\n",
    "print(all_articles_div.find('span').find_next_sibling())\n",
    "print()\n",
    "siblings = all_articles_div.find('span').find_next_siblings()\n",
    "for sibling in siblings:\n",
    "    print(sibling, '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each `bs4.element.ResultSet`, `bs4.element.Tag`,... can be used to create another BeautifulSoup object, using `BeautifulSoup(str(<bs4.element object>), features='html.parser')`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sibling_soup = BeautifulSoup(str(siblings[0]), 'html.parser')\n",
    "print(type(sibling_soup))\n",
    "print()\n",
    "print(sibling_soup)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get/Return all text from a `bs4.element.Tag` object, using `<bs4.element.Tag object>.text`, e.g. for an `<article>` tag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sibling_soup.find('article').text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get/Return and remove a specific item from a `bs4.element.ResultSet` using `<result set>.pop(<index>)` (default: last)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sibling = siblings.pop(1)\n",
    "print(sibling)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting a specific page from a Website where long lists of items are split in multiple pages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_specific_page(start_url: str, page=1):\n",
    "    \"\"\"Returns a specific page from a Website where long lists of items are split in multiple pages.\n",
    "    \"\"\"\n",
    "\n",
    "    if page > 1:\n",
    "        return start_url.split('&searchpage=')[0] + '&searchpage=' + str(page)\n",
    "    return start_url.split('&searchpage=')[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_specific_page(start_url, page)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_specific_page(start_url + '&searchpage=1'))\n",
    "print(get_specific_page(start_url + '&searchpage=1', 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the BeautifulSoup object corresponding to a specific page page from a Website where long lists of items are split in multiple pages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_next_soup(start_url: str, page=1):\n",
    "    \"\"\"Returns the BeautifulSoup object corresponding to a specific page\n",
    "    in case there are multiple pages that list objects of interest.\n",
    "    Parameters:\n",
    "    - start_url: the starting page/url of a multi-page list of objects\n",
    "    - page: the page number of a specific page of a multi-page list of objects\n",
    "    Essentially, get_next_soup() just returns get_soup(get_specific_page(start_url, page)),\n",
    "    i.e. converts the result of the call to get_specific_page(start_url, page), which is a string,\n",
    "    into a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_soup(get_specific_page(start_url, page))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_next_soup(start_url: str, page=1)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_soup = get_next_soup(start_url, 2)\n",
    "print(next_soup)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the BeautifulSoup object corresponding to a specific page page from a Website where long lists of items are split in multiple pages - the `selenium` version."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_next_soup_selenium(start_url: str, page=1):\n",
    "    \"\"\"Returns the BeautifulSoup object corresponding to a specific page\n",
    "    in case there are multiple pages that list objects of interest, using selenium instead of requests.\n",
    "    Parameters:\n",
    "    - start_url: the starting page/url of a multi-page list of objects\n",
    "    - page: the page number of a specific page of a multi-page list of objects\n",
    "    Essentially, get_next_soup() just returns get_soup_selenium(get_specific_page(start_url, page)),\n",
    "    i.e. converts the result of the call to get_specific_page(start_url, page), which is a string,\n",
    "    into a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_soup_selenium(get_specific_page(start_url, page))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_next_soup(start_url: str, page=1)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_soup = get_next_soup_selenium(start_url, 2)\n",
    "print(next_soup.find('article'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Web crawler that collects info about specific articles from Ultimate Classic Rock, implemented as a Python generator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crawl(url: str, max_pages=1):\n",
    "    \"\"\"Web crawler that collects info about specific articles from Ultimate Classic Rock,\n",
    "    implemented as a Python generator that yields BeautifulSoup objects (get_next_soup() or get_next_soup_selenium())\n",
    "    from multi-page movie lists.\n",
    "    Parameters: the url of the starting page and the max number of pages to crawl in case of multi-page lists.\n",
    "    \"\"\"\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        yield get_next_soup_selenium(url, page + 1)\n",
    "        page += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `crawl(url: str, max_pages=1)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_soup = crawl(start_url, 3)\n",
    "while True:\n",
    "    try:\n",
    "        s = next(next_soup)\n",
    "        for article in s.find_all('article')[:-1]:\n",
    "            print(article.find('div', {'class': 'content'}).find('a').text)\n",
    "            print()\n",
    "    except StopIteration:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A crawler that returns structured information about articles related to Paul McCartney from Ultimate Classic Rock:\n",
    "    - article title\n",
    "    - article author\n",
    "    - article date\n",
    "    - featured image (URL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_article_info_list(start_url: str, max_pages=1):\n",
    "    \"\"\"\n",
    "    Returns structured information about articles related to Paul McCartney from a multi-page article list.\n",
    "    :param start_url: the url of the starting page of a multi-page article list\n",
    "    :param max_pages: the max number of pages to crawl\n",
    "    :return: a list of tuples of info-items about the articles from a multi-page article list\n",
    "    Creates and uses the following data:\n",
    "    - article title\n",
    "    - article author\n",
    "    - article date\n",
    "    - featured image (URL)\n",
    "    \"\"\"\n",
    "\n",
    "    article_info_list = []\n",
    "    next_soup = crawl(start_url, max_pages)\n",
    "    while True:\n",
    "        try:\n",
    "            s = next(next_soup)\n",
    "            for article in s.find_all('article')[:-1]:\n",
    "                div_image = article.findNext('div', {'class': 'article-image-wrapper'})\n",
    "                div_content = article.findNext('div', {'class': 'content'})\n",
    "                featured_image_url = div_image.findNext('a').attrs['data-image']\n",
    "                article_title = div_content.find('a').text\n",
    "                article_date = div_content.find_next('time').text\n",
    "                article_author = div_content.find_next('em').text.lstrip(' by  ')\n",
    "                article_info_list.append((article_title, article_author, article_date, featured_image_url))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return article_info_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_articles_info(start_url: str, max_pages=1)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article_info_list = get_article_info_list(start_url, 3)\n",
    "for article_info in article_info_list:\n",
    "    print(article_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the info returned by the crawler in a `csv` file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = DATA_DIR / 'articles.csv'\n",
    "header_row = ['Title', 'Author', 'Date', 'Featured image']\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:  # newline: avoid blank rows; encoding: enable ш,š...\n",
    "    out = csv.writer(f)\n",
    "    out.writerow(header_row)\n",
    "    out.writerows(article_info_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And show the `csv` file in Pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "article_info_table = pd.read_csv(csv_file)\n",
    "article_info_table"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
